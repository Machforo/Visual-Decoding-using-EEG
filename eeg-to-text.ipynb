{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9994768,"sourceType":"datasetVersion","datasetId":6151589},{"sourceId":10002286,"sourceType":"datasetVersion","datasetId":6156793}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n'''\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n'''        \n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T14:08:44.397087Z","iopub.execute_input":"2024-11-24T14:08:44.397389Z","iopub.status.idle":"2024-11-24T14:08:45.338509Z","shell.execute_reply.started":"2024-11-24T14:08:44.397359Z","shell.execute_reply":"2024-11-24T14:08:45.337470Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"\"\\nimport os\\nfor dirname, _, filenames in os.walk('/kaggle/input'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))\\n\""},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"from transformers import BlipForConditionalGeneration, BlipProcessor\nimport torch\nimport torch.nn as nn  # Make sure to import nn module\nimport torch.optim as optim\nimport numpy as np\n\n# Load BLIP model and processor\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T14:10:49.748617Z","iopub.execute_input":"2024-11-24T14:10:49.748947Z","iopub.status.idle":"2024-11-24T14:10:51.359430Z","shell.execute_reply.started":"2024-11-24T14:10:49.748919Z","shell.execute_reply":"2024-11-24T14:10:51.358462Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom tqdm import tqdm\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\n\n# Load your model\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to('cuda')\n\n# Load your EEG embeddings\nmapped_embeddings = np.load('/kaggle/input/modified-embeddings/modified_eeg_embeddings.npy')  # Load the preprocessed EEG embeddings\nmapped_embeddings = torch.tensor(mapped_embeddings).float().to('cuda')\n\n# Set batch size for processing\nbatch_size = 32\n\n# Check feature dimensions and prepare reshaping\nbatch_size, feature_dim = mapped_embeddings.shape\n\n# Calculate height and width dynamically based on feature_dim (without assuming a square shape)\nheight = None\nwidth = None\n\n# Loop through possible height values and calculate width\nfor h in range(1, int(feature_dim**0.5) + 1):\n    if feature_dim % h == 0:\n        height = h\n        width = feature_dim // h\n        break\n\n# If we can't find a valid height/width pair, raise an error\nif height is None or width is None:\n    raise ValueError(f\"Feature dimension ({feature_dim}) cannot be reshaped into valid height and width.\")\n\n# Reshape mapped EEG embeddings to 4D tensor (batch_size, 1, height, width)\nreshaped_embeddings = mapped_embeddings.view(batch_size, 1, height, width)\n\n# Repeat the single channel into 3 channels to mimic RGB-like input (required by the vision model)\nreshaped_embeddings = reshaped_embeddings.repeat(1, 3, 1, 1)  # Convert [batch_size, 1, height, width] to [batch_size, 3, height, width]\n\n# Pad the input to make sure it is large enough for the convolutional layers\n# For example, padding to (32, 32) or larger if necessary\npadding = (0, max(0, 32 - width), 0, max(0, 32 - height))  # Pad the width and height to 32 if necessary\nreshaped_embeddings = torch.nn.functional.pad(reshaped_embeddings, padding)\n\n# Generate text descriptions with progress bar\ngenerated_texts = []\nwith tqdm(total=len(mapped_embeddings), desc=\"Generating Text from EEG Embeddings\") as progress_bar:\n    for i in range(0, len(mapped_embeddings), batch_size):\n        batch = reshaped_embeddings[i:i + batch_size]  # Slice the batch\n\n        # Ensure the batch has the correct shape for the vision model (batch_size, 3, height, width)\n        pixel_values = batch  # Now the batch is in the expected 4D shape [batch_size, 3, height, width]\n        \n        # Generate text using the model\n        outputs = model.generate(pixel_values=pixel_values)  # Pass pixel values to generate text\n        \n        # Decode and store the generated text\n        for output in outputs:\n            generated_texts.append(model.decode(output, skip_special_tokens=True))\n\n        progress_bar.update(batch_size)  # Update progress bar\n\n# Save generated texts to a file\nnp.save('eeg_generated_texts_blip.npy', generated_texts)\nprint(\"Generated texts saved!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T22:13:41.539447Z","iopub.execute_input":"2024-11-23T22:13:41.540151Z","iopub.status.idle":"2024-11-23T22:13:43.591235Z","shell.execute_reply.started":"2024-11-23T22:13:41.540120Z","shell.execute_reply":"2024-11-23T22:13:43.590020Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[30], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Pad the input to make sure it is large enough for the convolutional layers\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# For example, padding to (32, 32) or larger if necessary\u001b[39;00m\n\u001b[1;32m     43\u001b[0m padding \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m32\u001b[39m \u001b[38;5;241m-\u001b[39m width), \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m32\u001b[39m \u001b[38;5;241m-\u001b[39m height))  \u001b[38;5;66;03m# Pad the width and height to 32 if necessary\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m reshaped_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreshaped_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Generate text descriptions with progress bar\u001b[39;00m\n\u001b[1;32m     47\u001b[0m generated_texts \u001b[38;5;241m=\u001b[39m []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:4552\u001b[0m, in \u001b[0;36mpad\u001b[0;34m(input, pad, mode, value)\u001b[0m\n\u001b[1;32m   4545\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplicate\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   4546\u001b[0m             \u001b[38;5;66;03m# Use slow decomp whose backward will be in terms of index_put.\u001b[39;00m\n\u001b[1;32m   4547\u001b[0m             \u001b[38;5;66;03m# importlib is required because the import cannot be top level\u001b[39;00m\n\u001b[1;32m   4548\u001b[0m             \u001b[38;5;66;03m# (cycle) and cannot be nested (TS doesn't support)\u001b[39;00m\n\u001b[1;32m   4549\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch._decomp.decompositions\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39m_replication_pad(\n\u001b[1;32m   4550\u001b[0m                 \u001b[38;5;28minput\u001b[39m, pad\n\u001b[1;32m   4551\u001b[0m             )\n\u001b[0;32m-> 4552\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.03 GiB. GPU 0 has a total capacity of 14.74 GiB of which 616.12 MiB is free. Process 2562 has 14.14 GiB memory in use. Of the allocated memory 13.68 GiB is allocated by PyTorch, and 344.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 3.03 GiB. GPU 0 has a total capacity of 14.74 GiB of which 616.12 MiB is free. Process 2562 has 14.14 GiB memory in use. Of the allocated memory 13.68 GiB is allocated by PyTorch, and 344.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":30},{"cell_type":"code","source":"################################### Using a different model for EEG to text conversion ###################################################","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T14:12:14.968340Z","iopub.execute_input":"2024-11-24T14:12:14.968731Z","iopub.status.idle":"2024-11-24T14:12:14.972942Z","shell.execute_reply.started":"2024-11-24T14:12:14.968700Z","shell.execute_reply":"2024-11-24T14:12:14.972008Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"## EEG to Text Embedding Mapping\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tqdm import tqdm\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load EEG and text embeddings\neeg_embeddings = np.load('/kaggle/input/modified-embeddings/modified_eeg_embeddings.npy')\ntext_embeddings = np.load('/kaggle/input/modified-embeddings/modified_text_embeddings.npy')\n\n# Convert to PyTorch tensors\neeg_embeddings = torch.tensor(eeg_embeddings, dtype=torch.float32).to(device)\ntext_embeddings = torch.tensor(text_embeddings, dtype=torch.float32).to(device)\n\n# Dataset and DataLoader\ndataset = TensorDataset(eeg_embeddings, text_embeddings)\ndataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n\n# Define Mapping Network\nclass EEGToTextMapper(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(EEGToTextMapper, self).__init__()\n        self.network = nn.Sequential(\n            nn.Linear(input_dim, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, output_dim)\n        )\n    \n    def forward(self, eeg_embedding):\n        return self.network(eeg_embedding)\n\n# Model and Optimizer\ninput_dim = eeg_embeddings.shape[1]\noutput_dim = text_embeddings.shape[1]\nmodel = EEGToTextMapper(input_dim, output_dim).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.MSELoss()\n\n# Training Loop\nepochs = 10\nmodel.train()\nfor epoch in range(epochs):\n    epoch_loss = 0\n    with tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\") as progress_bar:\n        for eeg_batch, text_batch in progress_bar:\n            optimizer.zero_grad()\n            predicted_text_embedding = model(eeg_batch)\n            loss = criterion(predicted_text_embedding, text_batch)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n            progress_bar.set_postfix({\"Batch Loss\": loss.item()})\n    print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {epoch_loss/len(dataloader):.4f}\")\n\n# Save the mapping model\ntorch.save(model.state_dict(), 'eeg_to_text_mapping_model.pth')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T18:50:06.613004Z","iopub.execute_input":"2024-11-24T18:50:06.613604Z","iopub.status.idle":"2024-11-24T18:50:15.843453Z","shell.execute_reply.started":"2024-11-24T18:50:06.613557Z","shell.execute_reply":"2024-11-24T18:50:15.842590Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 259/259 [00:00<00:00, 284.41it/s, Batch Loss=0.00084] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Average Loss: 0.0008\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 259/259 [00:00<00:00, 289.89it/s, Batch Loss=0.000756]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10, Average Loss: 0.0008\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 259/259 [00:00<00:00, 285.58it/s, Batch Loss=0.000813]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10, Average Loss: 0.0008\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 259/259 [00:00<00:00, 279.02it/s, Batch Loss=0.000788]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10, Average Loss: 0.0008\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 259/259 [00:00<00:00, 277.51it/s, Batch Loss=0.000812]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10, Average Loss: 0.0007\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 259/259 [00:00<00:00, 288.49it/s, Batch Loss=0.000734]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10, Average Loss: 0.0007\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 259/259 [00:00<00:00, 285.44it/s, Batch Loss=0.000748]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10, Average Loss: 0.0007\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 259/259 [00:00<00:00, 287.75it/s, Batch Loss=0.000772]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10, Average Loss: 0.0007\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 259/259 [00:00<00:00, 287.54it/s, Batch Loss=0.00078] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10, Average Loss: 0.0007\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 259/259 [00:00<00:00, 285.26it/s, Batch Loss=0.000679]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10, Average Loss: 0.0007\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, TensorDataset\nimport numpy as np\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\nfrom tqdm import tqdm\nimport os\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load pre-trained text decoder (GPT-2)\ntext_decoder = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ntext_decoder.config.pad_token_id = text_decoder.config.eos_token_id\n\n# Ensure EEG embeddings are correctly loaded and processed\neeg_embeddings = np.load('/kaggle/input/modified-embeddings/modified_eeg_embeddings.npy')\neeg_embeddings = torch.tensor(eeg_embeddings, dtype=torch.float32).to(device)\n\n# Load trained mapping model\nmodel.load_state_dict(torch.load('/kaggle/working/eeg_to_text_mapping_model.pth', map_location=device))  # Load trained weights\nmodel.to(device)\nmodel.eval()\n\n# Generate mapped embeddings\nmapped_embeddings = []\nwith torch.no_grad():\n    for eeg_batch in DataLoader(TensorDataset(eeg_embeddings), batch_size=64):\n        eeg_batch = eeg_batch[0]\n        mapped_embeddings.append(model(eeg_batch.to(device)))\n\nmapped_embeddings = torch.cat(mapped_embeddings, dim=0)\n\n# Generate text for each mapped embedding with improved visualization\nos.makedirs(\"generated_texts_logs\", exist_ok=True)\ngenerated_texts = []\n\nprint(\"Starting text generation...\")\nwith tqdm(total=len(mapped_embeddings), desc=\"Processing EEG to Text\", unit=\"emb\") as pbar:\n    for i, embedding in enumerate(mapped_embeddings):\n        input_ids = tokenizer.encode(\"<|startoftext|>\", return_tensors=\"pt\").to(device)\n        attention_mask = torch.ones_like(input_ids)  # Explicit attention mask\n\n        # Generate text\n        outputs = text_decoder.generate(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            max_length=50,\n            num_return_sequences=1,\n            no_repeat_ngram_size=2,\n            temperature=0.7,\n            do_sample=True\n        )\n        text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        generated_texts.append(text)\n\n        # Add dynamic information to the progress bar\n        pbar.set_postfix({\n            \"Current Index\": i,\n            \"Generated Text Snippet\": text[:30],  # Display the first 30 characters\n        })\n\n        # Save intermediate results every 100 steps\n        if (i + 1) % 100 == 0:\n            np.save(f\"generated_texts_logs/intermediate_texts_{i + 1}.npy\", generated_texts)\n\n        pbar.update(1)\n\n# Save final results\nnp.save('generated_texts_from_eeg.npy', generated_texts)\nprint(\"Generated texts saved!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T18:54:21.401184Z","iopub.execute_input":"2024-11-24T18:54:21.401528Z","iopub.status.idle":"2024-11-24T18:55:47.819597Z","shell.execute_reply.started":"2024-11-24T18:54:21.401498Z","shell.execute_reply":"2024-11-24T18:55:47.817832Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/3239209276.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('/kaggle/working/eeg_to_text_mapping_model.pth', map_location=device))  # Load trained weights\n","output_type":"stream"},{"name":"stdout","text":"Starting text generation...\n","output_type":"stream"},{"name":"stderr","text":"Processing EEG to Text:   0%|          | 0/16540 [00:00<?, ?emb/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 0/16540 [00:00<?, ?emb/s, Current Index=0, Generated Text Snippet=<|startoftext|>|h1|\n\nProcessing EEG to Text:   0%|          | 1/16540 [00:00<2:00:04,  2.30emb/s, Current Index=0, Generated Text Snippet=<|startoftext|>|h1|\n\n<textarea]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 2/16540 [00:00<1:54:42,  2.40emb/s, Current Index=1, Generated Text Snippet=<|startoftext|>|<{left:0}> - <]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 2/16540 [00:01<1:54:42,  2.40emb/s, Current Index=2, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   0%|          | 3/16540 [00:01<1:53:02,  2.44emb/s, Current Index=2, Generated Text Snippet=<|startoftext|>\n\n<text> <count]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 4/16540 [00:01<1:52:58,  2.44emb/s, Current Index=3, Generated Text Snippet=<|startoftext|>|backspace|<<<e]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 5/16540 [00:02<1:52:23,  2.45emb/s, Current Index=4, Generated Text Snippet=<|startoftext|>|<textarea>+</t]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 6/16540 [00:02<1:51:58,  2.46emb/s, Current Index=5, Generated Text Snippet=<|startoftext|> <input type=\"t]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 7/16540 [00:02<1:52:04,  2.46emb/s, Current Index=6, Generated Text Snippet=<|startoftext|>|[<\\/end ofline]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 7/16540 [00:03<1:52:04,  2.46emb/s, Current Index=7, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   0%|          | 8/16540 [00:03<1:39:12,  2.78emb/s, Current Index=7, Generated Text Snippet=<|startoftext|>\n\n<html><head><]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 9/16540 [00:03<1:43:52,  2.65emb/s, Current Index=8, Generated Text Snippet=<|startoftext|>| |start of tex]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 10/16540 [00:03<1:46:56,  2.58emb/s, Current Index=9, Generated Text Snippet=<|startoftext|>|</startword> <]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 11/16540 [00:04<1:40:37,  2.74emb/s, Current Index=10, Generated Text Snippet=<|startoftext|>|</endofline> <]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 11/16540 [00:04<1:40:37,  2.74emb/s, Current Index=11, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   0%|          | 12/16540 [00:04<1:44:47,  2.63emb/s, Current Index=11, Generated Text Snippet=<|startoftext|>\n\n<text_type>Te]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 13/16540 [00:05<1:47:44,  2.56emb/s, Current Index=12, Generated Text Snippet=<|startoftext|>||<br><b>1st</b]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 14/16540 [00:05<1:35:18,  2.89emb/s, Current Index=13, Generated Text Snippet=<|startoftext|>|<linebreak|>\"] Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 14/16540 [00:05<1:35:18,  2.89emb/s, Current Index=14, Generated Text Snippet=<|startoftext|>|\n\n[^/]\n.\nProcessing EEG to Text:   0%|          | 15/16540 [00:05<1:40:42,  2.74emb/s, Current Index=14, Generated Text Snippet=<|startoftext|>|\n\n[^/]\n.\n-[\\\\+]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 16/16540 [00:06<1:45:03,  2.62emb/s, Current Index=15, Generated Text Snippet=<|startoftext|>|<table> <tr> |]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 17/16540 [00:06<1:47:16,  2.57emb/s, Current Index=16, Generated Text Snippet=<|startoftext|> | | <|endoflin]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 17/16540 [00:06<1:47:16,  2.57emb/s, Current Index=17, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   0%|          | 18/16540 [00:06<1:49:10,  2.52emb/s, Current Index=17, Generated Text Snippet=<|startoftext|>\n\n<title>Saving]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 19/16540 [00:07<1:50:19,  2.50emb/s, Current Index=18, Generated Text Snippet=<|startoftext|>/<\\/begin>\\u003]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 19/16540 [00:07<1:50:19,  2.50emb/s, Current Index=19, Generated Text Snippet=<|startoftext|>|<>\n\nProcessing EEG to Text:   0%|          | 20/16540 [00:07<1:50:32,  2.49emb/s, Current Index=19, Generated Text Snippet=<|startoftext|>|<>\n\n<text>\\r]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 21/16540 [00:08<1:53:31,  2.43emb/s, Current Index=20, Generated Text Snippet=<|startoftext|>|<span style=\"f]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 22/16540 [00:08<1:53:32,  2.42emb/s, Current Index=21, Generated Text Snippet=<|startoftext|>|</a> <a href=\"]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 22/16540 [00:09<1:53:32,  2.42emb/s, Current Index=22, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   0%|          | 23/16540 [00:09<1:53:28,  2.43emb/s, Current Index=22, Generated Text Snippet=<|startoftext|>\n\n<a href=\"http]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 23/16540 [00:09<1:53:28,  2.43emb/s, Current Index=23, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   0%|          | 24/16540 [00:09<1:44:40,  2.63emb/s, Current Index=23, Generated Text Snippet=<|startoftext|>\n\n<subtext>[0-9]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 24/16540 [00:09<1:44:40,  2.63emb/s, Current Index=24, Generated Text Snippet=<|startoftext|>|\n\nProcessing EEG to Text:   0%|          | 25/16540 [00:09<1:46:33,  2.58emb/s, Current Index=24, Generated Text Snippet=<|startoftext|>|\n\n<blockquote]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 26/16540 [00:10<1:48:13,  2.54emb/s, Current Index=25, Generated Text Snippet=<|startoftext|>| <text-align>!]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 26/16540 [00:10<1:48:13,  2.54emb/s, Current Index=26, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   0%|          | 27/16540 [00:10<1:49:56,  2.50emb/s, Current Index=26, Generated Text Snippet=<|startoftext|>\n\n<a href=\"http]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 27/16540 [00:11<1:49:56,  2.50emb/s, Current Index=27, Generated Text Snippet=<|startoftext|>|\n\nProcessing EEG to Text:   0%|          | 28/16540 [00:11<1:50:55,  2.48emb/s, Current Index=27, Generated Text Snippet=<|startoftext|>|\n\n<html><head>]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 29/16540 [00:11<1:51:22,  2.47emb/s, Current Index=28, Generated Text Snippet=<|startoftext|>|indexofline|ti]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 30/16540 [00:11<1:51:54,  2.46emb/s, Current Index=29, Generated Text Snippet=<|startoftext|>|<\\/start> <\\/d]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 30/16540 [00:12<1:51:54,  2.46emb/s, Current Index=30, Generated Text Snippet=<|startoftext|>\n\n</|'|\nProcessing EEG to Text:   0%|          | 31/16540 [00:12<1:52:04,  2.46emb/s, Current Index=30, Generated Text Snippet=<|startoftext|>\n\n</|'|\n.|-| |.]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 31/16540 [00:12<1:52:04,  2.46emb/s, Current Index=31, Generated Text Snippet=<|startoftext|>\n\n<\nProcessing EEG to Text:   0%|          | 32/16540 [00:12<1:52:07,  2.45emb/s, Current Index=31, Generated Text Snippet=<|startoftext|>\n\n<\n.[ {'text':]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 32/16540 [00:13<1:52:07,  2.45emb/s, Current Index=32, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   0%|          | 33/16540 [00:13<1:49:31,  2.51emb/s, Current Index=32, Generated Text Snippet=<|startoftext|>\n\n<textarea> |]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 34/16540 [00:13<1:50:48,  2.48emb/s, Current Index=33, Generated Text Snippet=<|startoftext|>| <title>Saving]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 34/16540 [00:13<1:50:48,  2.48emb/s, Current Index=34, Generated Text Snippet=<|startoftext|>/\\0\\1\\\\\\\n\nProcessing EEG to Text:   0%|          | 35/16540 [00:13<1:51:49,  2.46emb/s, Current Index=34, Generated Text Snippet=<|startoftext|>/\\0\\1\\\\\\\n\n\\\\$/|]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 35/16540 [00:14<1:51:49,  2.46emb/s, Current Index=35, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   0%|          | 36/16540 [00:14<1:52:24,  2.45emb/s, Current Index=35, Generated Text Snippet=<|startoftext|>\n\nYou can do th]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 36/16540 [00:14<1:52:24,  2.45emb/s, Current Index=36, Generated Text Snippet=<|startoftext|>|</a>\n\nProcessing EEG to Text:   0%|          | 37/16540 [00:14<1:52:51,  2.44emb/s, Current Index=36, Generated Text Snippet=<|startoftext|>|</a>\n\n<a href=]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 38/16540 [00:15<1:52:22,  2.45emb/s, Current Index=37, Generated Text Snippet=<|startoftext|>|[\\/en-us\\/]\",\"]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 38/16540 [00:15<1:52:22,  2.45emb/s, Current Index=38, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   0%|          | 39/16540 [00:15<1:52:04,  2.45emb/s, Current Index=38, Generated Text Snippet=<|startoftext|>\n\n[^(?:#\\s|\\d{4]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 39/16540 [00:15<1:52:04,  2.45emb/s, Current Index=39, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   0%|          | 40/16540 [00:15<1:51:19,  2.47emb/s, Current Index=39, Generated Text Snippet=<|startoftext|>\n\n-- ----------]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 41/16540 [00:16<1:51:03,  2.48emb/s, Current Index=40, Generated Text Snippet=<|startoftext|>|n</ul> </li> <]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 41/16540 [00:16<1:51:03,  2.48emb/s, Current Index=41, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   0%|          | 42/16540 [00:16<1:50:41,  2.48emb/s, Current Index=41, Generated Text Snippet=<|startoftext|>\n\n\\t\\tset $tw.u]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 42/16540 [00:17<1:50:41,  2.48emb/s, Current Index=42, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   0%|          | 43/16540 [00:17<1:51:02,  2.48emb/s, Current Index=42, Generated Text Snippet=<|startoftext|>\n\n<textarea> <?]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 44/16540 [00:17<1:51:06,  2.47emb/s, Current Index=43, Generated Text Snippet=<|startoftext|>| | | || (?:s||]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 45/16540 [00:17<1:51:05,  2.47emb/s, Current Index=44, Generated Text Snippet=<|startoftext|>, <|numwords|<t]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 45/16540 [00:18<1:51:05,  2.47emb/s, Current Index=45, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   0%|          | 46/16540 [00:18<1:54:04,  2.41emb/s, Current Index=45, Generated Text Snippet=<|startoftext|>\n\n<br>The [RFC]]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 46/16540 [00:18<1:54:04,  2.41emb/s, Current Index=46, Generated Text Snippet=<|startoftext|>\n\n<\\/begin>\n\n\nProcessing EEG to Text:   0%|          | 47/16540 [00:18<1:53:44,  2.42emb/s, Current Index=46, Generated Text Snippet=<|startoftext|>\n\n<\\/begin>\n\n\n\\]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 48/16540 [00:19<1:56:07,  2.37emb/s, Current Index=47, Generated Text Snippet=<|startoftext|>|<?php echo $_G]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 49/16540 [00:19<1:56:34,  2.36emb/s, Current Index=48, Generated Text Snippet=<|startoftext|> <|endofword|<l]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 49/16540 [00:20<1:56:34,  2.36emb/s, Current Index=49, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   0%|          | 50/16540 [00:20<1:55:40,  2.38emb/s, Current Index=49, Generated Text Snippet=<|startoftext|>\n\n<a href=\" htt]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 51/16540 [00:20<1:54:56,  2.39emb/s, Current Index=50, Generated Text Snippet=<|startoftext|>|<![CDATA[1]]>] Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 51/16540 [00:20<1:54:56,  2.39emb/s, Current Index=51, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   0%|          | 52/16540 [00:20<1:52:53,  2.43emb/s, Current Index=51, Generated Text Snippet=<|startoftext|>\n\n</table> <tab]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 53/16540 [00:21<1:52:07,  2.45emb/s, Current Index=52, Generated Text Snippet=<|startoftext|> <|endofline|>[]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 53/16540 [00:21<1:52:07,  2.45emb/s, Current Index=53, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   0%|          | 54/16540 [00:21<1:51:33,  2.46emb/s, Current Index=53, Generated Text Snippet=<|startoftext|>\n\n<endofline> <]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 54/16540 [00:22<1:51:33,  2.46emb/s, Current Index=54, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   0%|          | 55/16540 [00:22<1:51:15,  2.47emb/s, Current Index=54, Generated Text Snippet=<|startoftext|>\n\n<\\/start> <\\/]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 56/16540 [00:22<1:51:14,  2.47emb/s, Current Index=55, Generated Text Snippet=<|startoftext|> <|endofline|><]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 56/16540 [00:22<1:51:14,  2.47emb/s, Current Index=56, Generated Text Snippet=<|startoftext|><br>\n\nProcessing EEG to Text:   0%|          | 57/16540 [00:22<1:51:06,  2.47emb/s, Current Index=56, Generated Text Snippet=<|startoftext|><br>\n\n<link rel]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 58/16540 [00:23<1:51:34,  2.46emb/s, Current Index=57, Generated Text Snippet=<|startoftext|> <span>F:</span]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 58/16540 [00:23<1:51:34,  2.46emb/s, Current Index=58, Generated Text Snippet=<|startoftext|>|\n\nProcessing EEG to Text:   0%|          | 59/16540 [00:23<1:51:56,  2.45emb/s, Current Index=58, Generated Text Snippet=<|startoftext|>|\n\n<textarea> |]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 60/16540 [00:24<1:51:34,  2.46emb/s, Current Index=59, Generated Text Snippet=<|startoftext|><\\/structure>\\\"]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 60/16540 [00:24<1:51:34,  2.46emb/s, Current Index=60, Generated Text Snippet=<|startoftext|>|\\\n\nProcessing EEG to Text:   0%|          | 61/16540 [00:24<1:52:01,  2.45emb/s, Current Index=60, Generated Text Snippet=<|startoftext|>|\\\n\n\\t\\ttitle:]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 61/16540 [00:24<1:52:01,  2.45emb/s, Current Index=61, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   0%|          | 62/16540 [00:24<1:51:48,  2.46emb/s, Current Index=61, Generated Text Snippet=<|startoftext|>\n\n<\\/span>\\u003]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 62/16540 [00:25<1:51:48,  2.46emb/s, Current Index=62, Generated Text Snippet=<|startoftext|>|\n\nProcessing EEG to Text:   0%|          | 63/16540 [00:25<1:52:03,  2.45emb/s, Current Index=62, Generated Text Snippet=<|startoftext|>|\n\n<subtype|sub]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 64/16540 [00:25<1:51:33,  2.46emb/s, Current Index=63, Generated Text Snippet=<|startoftext|>|</head> <body>]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 65/16540 [00:26<1:51:26,  2.46emb/s, Current Index=64, Generated Text Snippet=<|startoftext|>| </span> <span]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 66/16540 [00:26<1:51:10,  2.47emb/s, Current Index=65, Generated Text Snippet=<|startoftext|>|<br /> </p> <p]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 67/16540 [00:26<1:52:37,  2.44emb/s, Current Index=66, Generated Text Snippet=<|startoftext|>|<length>:</len]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 67/16540 [00:27<1:52:37,  2.44emb/s, Current Index=67, Generated Text Snippet=<|startoftext|> /dev/sda\n\n#\nProcessing EEG to Text:   0%|          | 68/16540 [00:27<1:52:59,  2.43emb/s, Current Index=67, Generated Text Snippet=<|startoftext|> /dev/sda\n\n#\n (]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 69/16540 [00:27<1:53:27,  2.42emb/s, Current Index=68, Generated Text Snippet=<|startoftext|>|</table> <tabl]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 70/16540 [00:28<1:56:39,  2.35emb/s, Current Index=69, Generated Text Snippet=<|startoftext|>|</|<!--><\\/stu]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 71/16540 [00:28<1:54:49,  2.39emb/s, Current Index=70, Generated Text Snippet=<|startoftext|>|<![CDATA[#^\\/]]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 71/16540 [00:28<1:54:49,  2.39emb/s, Current Index=71, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   0%|          | 72/16540 [00:28<1:32:20,  2.97emb/s, Current Index=71, Generated Text Snippet=<|startoftext|>\n\n<textarea> <<]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 73/16540 [00:29<1:38:47,  2.78emb/s, Current Index=72, Generated Text Snippet=<|startoftext|>:</p> <p class=]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 74/16540 [00:29<1:42:37,  2.67emb/s, Current Index=73, Generated Text Snippet=<|startoftext|> <|endofsentenc]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 75/16540 [00:30<1:45:39,  2.60emb/s, Current Index=74, Generated Text Snippet=<|startoftext|> <a href=\"http:]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 76/16540 [00:30<1:48:16,  2.53emb/s, Current Index=75, Generated Text Snippet=<|startoftext|>|endofline||><|]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 76/16540 [00:30<1:48:16,  2.53emb/s, Current Index=76, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   0%|          | 77/16540 [00:30<1:49:10,  2.51emb/s, Current Index=76, Generated Text Snippet=<|startoftext|>\n\n<title>This i]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 78/16540 [00:31<1:50:10,  2.49emb/s, Current Index=77, Generated Text Snippet=<|startoftext|>|<![CDATA[1]]|\\]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 78/16540 [00:31<1:50:10,  2.49emb/s, Current Index=78, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   0%|          | 79/16540 [00:31<1:50:43,  2.48emb/s, Current Index=78, Generated Text Snippet=<|startoftext|>\n\n<table id=\"at]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 80/16540 [00:32<1:51:06,  2.47emb/s, Current Index=79, Generated Text Snippet=<|startoftext|>|<textarea>\\</t]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 81/16540 [00:32<1:34:35,  2.90emb/s, Current Index=80, Generated Text Snippet=<|startoftext|> <title>The Old]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   0%|          | 81/16540 [00:32<1:34:35,  2.90emb/s, Current Index=81, Generated Text Snippet=<|startoftext|>|\\\n\nProcessing EEG to Text:   0%|          | 82/16540 [00:32<1:39:51,  2.75emb/s, Current Index=81, Generated Text Snippet=<|startoftext|>|\\\n\n<\\/endofthe]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 83/16540 [00:33<1:44:00,  2.64emb/s, Current Index=82, Generated Text Snippet=<|startoftext|>|back to text|] Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 84/16540 [00:33<1:46:39,  2.57emb/s, Current Index=83, Generated Text Snippet=<|startoftext|> <span>My Name<]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 85/16540 [00:33<1:48:09,  2.54emb/s, Current Index=84, Generated Text Snippet=<|startoftext|>|<textarea><img]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 86/16540 [00:34<1:49:47,  2.50emb/s, Current Index=85, Generated Text Snippet=<|startoftext|>|<linebreak>/><]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 87/16540 [00:34<1:50:39,  2.48emb/s, Current Index=86, Generated Text Snippet=<|startoftext|>|<table name=\"t]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 88/16540 [00:35<1:50:58,  2.47emb/s, Current Index=87, Generated Text Snippet=<|startoftext|>|endofword|</|] Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 89/16540 [00:35<1:51:05,  2.47emb/s, Current Index=88, Generated Text Snippet=<|startoftext|>|| | ||End of t]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 90/16540 [00:35<1:51:44,  2.45emb/s, Current Index=89, Generated Text Snippet=<|startoftext|>|<font style=\"b]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 90/16540 [00:36<1:51:44,  2.45emb/s, Current Index=90, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 91/16540 [00:36<1:51:41,  2.45emb/s, Current Index=90, Generated Text Snippet=<|startoftext|>\n\n[$(this[0]))]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 92/16540 [00:36<1:51:28,  2.46emb/s, Current Index=91, Generated Text Snippet=<|startoftext|>|<textarea></te]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 92/16540 [00:37<1:51:28,  2.46emb/s, Current Index=92, Generated Text Snippet=<|startoftext|>\n\n</|\nProcessing EEG to Text:   1%|          | 93/16540 [00:37<1:51:47,  2.45emb/s, Current Index=92, Generated Text Snippet=<|startoftext|>\n\n</|\n.| |<<|en]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 94/16540 [00:37<1:52:04,  2.45emb/s, Current Index=93, Generated Text Snippet=<|startoftext|>|<\\/startline|v]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 94/16540 [00:38<1:52:04,  2.45emb/s, Current Index=94, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 95/16540 [00:38<1:53:25,  2.42emb/s, Current Index=94, Generated Text Snippet=<|startoftext|>\n\n<b>The word ']Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 96/16540 [00:38<1:52:36,  2.43emb/s, Current Index=95, Generated Text Snippet=<|startoftext|>|<textarea styl]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 96/16540 [00:38<1:52:36,  2.43emb/s, Current Index=96, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 97/16540 [00:38<1:52:49,  2.43emb/s, Current Index=96, Generated Text Snippet=<|startoftext|>\n\n<text>[{text}]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 97/16540 [00:39<1:52:49,  2.43emb/s, Current Index=97, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 98/16540 [00:39<1:52:27,  2.44emb/s, Current Index=97, Generated Text Snippet=<|startoftext|>\n\n|<endofline|>]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 98/16540 [00:39<1:52:27,  2.44emb/s, Current Index=98, Generated Text Snippet=<|startoftext|> (<||<)\n\nProcessing EEG to Text:   1%|          | 99/16540 [00:39<1:52:11,  2.44emb/s, Current Index=98, Generated Text Snippet=<|startoftext|> (<||<)\n\n<=<<<>]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 99/16540 [00:40<1:52:11,  2.44emb/s, Current Index=99, Generated Text Snippet=<|startoftext|>|<\\/start>\n\nProcessing EEG to Text:   1%|          | 100/16540 [00:40<1:53:58,  2.40emb/s, Current Index=99, Generated Text Snippet=<|startoftext|>|<\\/start>\n\n<p>]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 100/16540 [00:40<1:53:58,  2.40emb/s, Current Index=100, Generated Text Snippet=<|startoftext|>/\n\nProcessing EEG to Text:   1%|          | 101/16540 [00:40<1:53:00,  2.42emb/s, Current Index=100, Generated Text Snippet=<|startoftext|>/\n\n<![CDATA[0]I]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 101/16540 [00:40<1:53:00,  2.42emb/s, Current Index=101, Generated Text Snippet=<|startoftext|> |\n\nProcessing EEG to Text:   1%|          | 102/16540 [00:40<1:53:12,  2.42emb/s, Current Index=101, Generated Text Snippet=<|startoftext|> |\n\n<![CDATA[ \"]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 103/16540 [00:41<1:53:19,  2.42emb/s, Current Index=102, Generated Text Snippet=<|startoftext|>||endofline|\"]  Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 104/16540 [00:41<1:52:42,  2.43emb/s, Current Index=103, Generated Text Snippet=<|startoftext|>|nth-of-type [\\]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 104/16540 [00:42<1:52:42,  2.43emb/s, Current Index=104, Generated Text Snippet=<|startoftext|>|<\\/start>\n\nProcessing EEG to Text:   1%|          | 105/16540 [00:42<1:52:46,  2.43emb/s, Current Index=104, Generated Text Snippet=<|startoftext|>|<\\/start>\n\n<sp]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 106/16540 [00:42<1:52:40,  2.43emb/s, Current Index=105, Generated Text Snippet=<|startoftext|>| <|endoftitle|]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 106/16540 [00:42<1:52:40,  2.43emb/s, Current Index=106, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 107/16540 [00:42<1:52:36,  2.43emb/s, Current Index=106, Generated Text Snippet=<|startoftext|>\n\n[0x000a00] [R]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 108/16540 [00:43<1:52:21,  2.44emb/s, Current Index=107, Generated Text Snippet=<|startoftext|> <pre>|b</pre><]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 109/16540 [00:43<1:52:24,  2.44emb/s, Current Index=108, Generated Text Snippet=<|startoftext|>|<\\/start oftex]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 109/16540 [00:44<1:52:24,  2.44emb/s, Current Index=109, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 110/16540 [00:44<1:51:22,  2.46emb/s, Current Index=109, Generated Text Snippet=<|startoftext|>\n\n\\t\\tif(text)]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 110/16540 [00:44<1:51:22,  2.46emb/s, Current Index=110, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 111/16540 [00:44<1:51:05,  2.46emb/s, Current Index=110, Generated Text Snippet=<|startoftext|>\n\n[01:16:37]EMO]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 111/16540 [00:44<1:51:05,  2.46emb/s, Current Index=111, Generated Text Snippet=<|startoftext|> |\n\nProcessing EEG to Text:   1%|          | 112/16540 [00:44<1:51:09,  2.46emb/s, Current Index=111, Generated Text Snippet=<|startoftext|> |\n\n<a href=\"ht]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 112/16540 [00:45<1:51:09,  2.46emb/s, Current Index=112, Generated Text Snippet=<|startoftext|>|\n\n<br/>\n.\nProcessing EEG to Text:   1%|          | 113/16540 [00:45<1:51:00,  2.47emb/s, Current Index=112, Generated Text Snippet=<|startoftext|>|\n\n<br/>\n.\n ( \"]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 114/16540 [00:45<1:50:53,  2.47emb/s, Current Index=113, Generated Text Snippet=<|startoftext|>|<\\/start\\\"><\\/]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 115/16540 [00:46<1:51:04,  2.46emb/s, Current Index=114, Generated Text Snippet=<|startoftext|> |start||end|<e]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 116/16540 [00:46<1:51:23,  2.46emb/s, Current Index=115, Generated Text Snippet=<|startoftext|> <a href=\"javas]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 117/16540 [00:47<1:51:34,  2.45emb/s, Current Index=116, Generated Text Snippet=<|startoftext|>|</a></li> <li>]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 117/16540 [00:47<1:51:34,  2.45emb/s, Current Index=117, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 118/16540 [00:47<1:51:20,  2.46emb/s, Current Index=117, Generated Text Snippet=<|startoftext|>\n\n<line>\\<strin]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 119/16540 [00:47<1:51:04,  2.46emb/s, Current Index=118, Generated Text Snippet=<|startoftext|>|<br /> <br /><]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 119/16540 [00:48<1:51:04,  2.46emb/s, Current Index=119, Generated Text Snippet=<|startoftext|>\n\n</table>\n\n\nProcessing EEG to Text:   1%|          | 120/16540 [00:48<1:53:27,  2.41emb/s, Current Index=119, Generated Text Snippet=<|startoftext|>\n\n</table>\n\n\n<t]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 121/16540 [00:48<1:52:33,  2.43emb/s, Current Index=120, Generated Text Snippet=<|startoftext|> | <|endofdate|]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 121/16540 [00:49<1:52:33,  2.43emb/s, Current Index=121, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 122/16540 [00:49<1:53:07,  2.42emb/s, Current Index=121, Generated Text Snippet=<|startoftext|>\n\n<endofline>|]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 122/16540 [00:49<1:53:07,  2.42emb/s, Current Index=122, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 123/16540 [00:49<1:52:21,  2.44emb/s, Current Index=122, Generated Text Snippet=<|startoftext|>\n\n[01:11:29]SAY]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 123/16540 [00:49<1:52:21,  2.44emb/s, Current Index=123, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 124/16540 [00:49<1:52:03,  2.44emb/s, Current Index=123, Generated Text Snippet=<|startoftext|>\n\n<textarea cla]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 124/16540 [00:50<1:52:03,  2.44emb/s, Current Index=124, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 125/16540 [00:50<1:52:29,  2.43emb/s, Current Index=124, Generated Text Snippet=<|startoftext|>\n\n<text-body>Na]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 125/16540 [00:50<1:52:29,  2.43emb/s, Current Index=125, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 126/16540 [00:50<1:39:39,  2.75emb/s, Current Index=125, Generated Text Snippet=<|startoftext|>\n\n<*/> (this is]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 126/16540 [00:51<1:39:39,  2.75emb/s, Current Index=126, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 127/16540 [00:51<1:46:18,  2.57emb/s, Current Index=126, Generated Text Snippet=<|startoftext|>\n\n<a href= \"htt]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 128/16540 [00:51<1:48:58,  2.51emb/s, Current Index=127, Generated Text Snippet=<|startoftext|> <title>Curious]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 128/16540 [00:51<1:48:58,  2.51emb/s, Current Index=128, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 129/16540 [00:51<1:51:17,  2.46emb/s, Current Index=128, Generated Text Snippet=<|startoftext|>\n\n<textarea>{{{]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 130/16540 [00:52<1:51:27,  2.45emb/s, Current Index=129, Generated Text Snippet=<|startoftext|> | | 1 | 2 | 3] Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 131/16540 [00:52<1:51:33,  2.45emb/s, Current Index=130, Generated Text Snippet=<|startoftext|> | <|endofsecti]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 132/16540 [00:52<1:44:05,  2.63emb/s, Current Index=131, Generated Text Snippet=<|startoftext|> <!DOCTYPE html]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 132/16540 [00:53<1:44:05,  2.63emb/s, Current Index=132, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 133/16540 [00:53<1:45:55,  2.58emb/s, Current Index=132, Generated Text Snippet=<|startoftext|>\n\n<table> <tr>]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 133/16540 [00:53<1:45:55,  2.58emb/s, Current Index=133, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 134/16540 [00:53<1:46:56,  2.56emb/s, Current Index=133, Generated Text Snippet=<|startoftext|>\n\n</span></div>]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 135/16540 [00:54<1:47:58,  2.53emb/s, Current Index=134, Generated Text Snippet=<|startoftext|>|<textarea></te]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 135/16540 [00:54<1:47:58,  2.53emb/s, Current Index=135, Generated Text Snippet=<|startoftext|>|\n\n<br />\nProcessing EEG to Text:   1%|          | 136/16540 [00:54<1:49:02,  2.51emb/s, Current Index=135, Generated Text Snippet=<|startoftext|>|\n\n<br />\n. . .]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 137/16540 [00:55<1:49:25,  2.50emb/s, Current Index=136, Generated Text Snippet=<|startoftext|>|<title><img sr]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 138/16540 [00:55<1:49:45,  2.49emb/s, Current Index=137, Generated Text Snippet=<|startoftext|>|[<\\/start\\/col]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 139/16540 [00:55<1:50:06,  2.48emb/s, Current Index=138, Generated Text Snippet=<|startoftext|>|<html><body>{{]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 140/16540 [00:56<1:50:33,  2.47emb/s, Current Index=139, Generated Text Snippet=<|startoftext|> 1st line of te]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 140/16540 [00:56<1:50:33,  2.47emb/s, Current Index=140, Generated Text Snippet=<|startoftext|>| endofline|\n\nProcessing EEG to Text:   1%|          | 141/16540 [00:56<1:50:23,  2.48emb/s, Current Index=140, Generated Text Snippet=<|startoftext|>| endofline|\n\n|]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 142/16540 [00:57<1:50:07,  2.48emb/s, Current Index=141, Generated Text Snippet=<|startoftext|>|w|a|p|s|t|v)|y]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 143/16540 [00:57<1:51:06,  2.46emb/s, Current Index=142, Generated Text Snippet=<|startoftext|>|<p><b>This is<]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 144/16540 [00:57<1:51:15,  2.46emb/s, Current Index=143, Generated Text Snippet=<|startoftext|>|nose|[](|tilde]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 144/16540 [00:58<1:51:15,  2.46emb/s, Current Index=144, Generated Text Snippet=<|startoftext|>|~|<text>\n\nProcessing EEG to Text:   1%|          | 145/16540 [00:58<1:53:59,  2.40emb/s, Current Index=144, Generated Text Snippet=<|startoftext|>|~|<text>\n\n<spa]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 145/16540 [00:58<1:53:59,  2.40emb/s, Current Index=145, Generated Text Snippet=<|startoftext|>&=&&r\n\nProcessing EEG to Text:   1%|          | 146/16540 [00:58<1:53:08,  2.42emb/s, Current Index=145, Generated Text Snippet=<|startoftext|>&=&&r\n\n[0x000a8]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 147/16540 [00:59<1:52:10,  2.44emb/s, Current Index=146, Generated Text Snippet=<|startoftext|>|endofdocument|]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 148/16540 [00:59<1:51:35,  2.45emb/s, Current Index=147, Generated Text Snippet=<|startoftext|>|t|<a href=\"htt]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 148/16540 [00:59<1:51:35,  2.45emb/s, Current Index=148, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 149/16540 [00:59<1:51:10,  2.46emb/s, Current Index=148, Generated Text Snippet=<|startoftext|>\n\n<input type=\"]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 150/16540 [01:00<1:51:32,  2.45emb/s, Current Index=149, Generated Text Snippet=<|startoftext|>|<</endofline>] Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 151/16540 [01:00<1:50:49,  2.46emb/s, Current Index=150, Generated Text Snippet=<|startoftext|>| <?php $vars[0]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 151/16540 [01:01<1:50:49,  2.46emb/s, Current Index=151, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 152/16540 [01:01<1:50:26,  2.47emb/s, Current Index=151, Generated Text Snippet=<|startoftext|>\n\nThe first lin]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 153/16540 [01:01<1:50:41,  2.47emb/s, Current Index=152, Generated Text Snippet=<|startoftext|>|</| | | <| end]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 153/16540 [01:01<1:50:41,  2.47emb/s, Current Index=153, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 154/16540 [01:01<1:51:01,  2.46emb/s, Current Index=153, Generated Text Snippet=<|startoftext|>\n\n|<textarea>|]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 154/16540 [01:02<1:51:01,  2.46emb/s, Current Index=154, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 155/16540 [01:02<1:51:16,  2.45emb/s, Current Index=154, Generated Text Snippet=<|startoftext|>\n\n<textarea> </]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 155/16540 [01:02<1:51:16,  2.45emb/s, Current Index=155, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 156/16540 [01:02<1:50:43,  2.47emb/s, Current Index=155, Generated Text Snippet=<|startoftext|>\n\n<div> <h1>Wha]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 157/16540 [01:03<1:51:39,  2.45emb/s, Current Index=156, Generated Text Snippet=<|startoftext|>|<\\/start> <spa]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 158/16540 [01:03<1:51:43,  2.44emb/s, Current Index=157, Generated Text Snippet=<|startoftext|>|-\\.+\\.[\\w|\\d+]]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 158/16540 [01:03<1:51:43,  2.44emb/s, Current Index=158, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 159/16540 [01:03<1:28:32,  3.08emb/s, Current Index=158, Generated Text Snippet=<|startoftext|>\n\n[ -e \"| | \" |]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 159/16540 [01:04<1:28:32,  3.08emb/s, Current Index=159, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 160/16540 [01:04<1:36:07,  2.84emb/s, Current Index=159, Generated Text Snippet=<|startoftext|>\n\n<div class=\"u]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 160/16540 [01:04<1:36:07,  2.84emb/s, Current Index=160, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 161/16540 [01:04<1:41:04,  2.70emb/s, Current Index=160, Generated Text Snippet=<|startoftext|>\n\nThe following]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 161/16540 [01:04<1:41:04,  2.70emb/s, Current Index=161, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 162/16540 [01:04<1:44:00,  2.62emb/s, Current Index=161, Generated Text Snippet=<|startoftext|>\n\n<a href=\"http]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 163/16540 [01:05<1:46:40,  2.56emb/s, Current Index=162, Generated Text Snippet=<|startoftext|>|</span></div>] Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 164/16540 [01:05<1:47:59,  2.53emb/s, Current Index=163, Generated Text Snippet=<|startoftext|> | | # | <|firs]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 164/16540 [01:06<1:47:59,  2.53emb/s, Current Index=164, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 165/16540 [01:06<1:49:26,  2.49emb/s, Current Index=164, Generated Text Snippet=<|startoftext|>\n\n<textarea>]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 165/16540 [01:06<1:49:26,  2.49emb/s, Current Index=165, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 166/16540 [01:06<1:49:55,  2.48emb/s, Current Index=165, Generated Text Snippet=<|startoftext|>\n\n\\t\\tif(argume]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 166/16540 [01:06<1:49:55,  2.48emb/s, Current Index=166, Generated Text Snippet=<|startoftext|>|<filter>\n\nProcessing EEG to Text:   1%|          | 167/16540 [01:06<1:50:23,  2.47emb/s, Current Index=166, Generated Text Snippet=<|startoftext|>|<filter>\n\n<lis]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 167/16540 [01:07<1:50:23,  2.47emb/s, Current Index=167, Generated Text Snippet=<|startoftext|>\n\n<\\/span>\n\n\nProcessing EEG to Text:   1%|          | 168/16540 [01:07<1:26:51,  3.14emb/s, Current Index=167, Generated Text Snippet=<|startoftext|>\n\n<\\/span>\n\n\n</]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 168/16540 [01:07<1:26:51,  3.14emb/s, Current Index=168, Generated Text Snippet=<|startoftext|>|\n\nProcessing EEG to Text:   1%|          | 169/16540 [01:07<1:33:45,  2.91emb/s, Current Index=168, Generated Text Snippet=<|startoftext|>|\n\n<a href=\"htt]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 169/16540 [01:07<1:33:45,  2.91emb/s, Current Index=169, Generated Text Snippet=<|startoftext|>|\n\nProcessing EEG to Text:   1%|          | 170/16540 [01:07<1:39:00,  2.76emb/s, Current Index=169, Generated Text Snippet=<|startoftext|>|\n\n<textarea id]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 170/16540 [01:08<1:39:00,  2.76emb/s, Current Index=170, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 171/16540 [01:08<1:45:05,  2.60emb/s, Current Index=170, Generated Text Snippet=<|startoftext|>\n\n<textarea> |<]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 171/16540 [01:08<1:45:05,  2.60emb/s, Current Index=171, Generated Text Snippet=<|startoftext|>\n\n[/endnote]\nProcessing EEG to Text:   1%|          | 172/16540 [01:08<1:29:53,  3.03emb/s, Current Index=171, Generated Text Snippet=<|startoftext|>\n\n[/endnote]\n—]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 172/16540 [01:08<1:29:53,  3.03emb/s, Current Index=172, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 173/16540 [01:08<1:35:21,  2.86emb/s, Current Index=172, Generated Text Snippet=<|startoftext|>\n\n<textarea> <|]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 174/16540 [01:09<1:39:45,  2.73emb/s, Current Index=173, Generated Text Snippet=<|startoftext|>|</text> </td>] Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 174/16540 [01:09<1:39:45,  2.73emb/s, Current Index=174, Generated Text Snippet=<|startoftext|>\n\n<*/>\n.\nProcessing EEG to Text:   1%|          | 175/16540 [01:09<1:42:57,  2.65emb/s, Current Index=174, Generated Text Snippet=<|startoftext|>\n\n<*/>\n.\n:endof]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 176/16540 [01:10<1:45:26,  2.59emb/s, Current Index=175, Generated Text Snippet=<|startoftext|>|texts</a></br>]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 177/16540 [01:10<1:47:18,  2.54emb/s, Current Index=176, Generated Text Snippet=<|startoftext|>|<font size=\\\"1]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 178/16540 [01:10<1:48:20,  2.52emb/s, Current Index=177, Generated Text Snippet=<|startoftext|> (0-9) | <|endo]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 179/16540 [01:11<1:49:14,  2.50emb/s, Current Index=178, Generated Text Snippet=<|startoftext|> <!DOCTYPE html]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 179/16540 [01:11<1:49:14,  2.50emb/s, Current Index=179, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 180/16540 [01:11<1:49:43,  2.48emb/s, Current Index=179, Generated Text Snippet=<|startoftext|>\n\n<a href=\"http]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 181/16540 [01:12<1:50:09,  2.48emb/s, Current Index=180, Generated Text Snippet=<|startoftext|>||-| (2|1)||2.|]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 182/16540 [01:12<1:49:55,  2.48emb/s, Current Index=181, Generated Text Snippet=<|startoftext|> <|endoftitle>] Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 182/16540 [01:13<1:49:55,  2.48emb/s, Current Index=182, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 183/16540 [01:13<1:49:37,  2.49emb/s, Current Index=182, Generated Text Snippet=<|startoftext|>\n\n<$list filter]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 183/16540 [01:13<1:49:37,  2.49emb/s, Current Index=183, Generated Text Snippet=<|startoftext|>/etc/fstab\n\nProcessing EEG to Text:   1%|          | 184/16540 [01:13<1:50:03,  2.48emb/s, Current Index=183, Generated Text Snippet=<|startoftext|>/etc/fstab\n\n<en]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 185/16540 [01:13<1:50:41,  2.46emb/s, Current Index=184, Generated Text Snippet=<|startoftext|>|<br> <br/> <di]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 185/16540 [01:13<1:50:41,  2.46emb/s, Current Index=185, Generated Text Snippet=<|startoftext|>]               Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 186/16540 [01:14<1:50:41,  2.46emb/s, Current Index=186, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 187/16540 [01:14<1:26:09,  3.16emb/s, Current Index=186, Generated Text Snippet=<|startoftext|>\n\n<textarea> <h]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 187/16540 [01:14<1:26:09,  3.16emb/s, Current Index=187, Generated Text Snippet=<|startoftext|>|\n\nProcessing EEG to Text:   1%|          | 188/16540 [01:14<1:32:12,  2.96emb/s, Current Index=187, Generated Text Snippet=<|startoftext|>|\n\n<textarea> <]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 188/16540 [01:15<1:32:12,  2.96emb/s, Current Index=188, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 189/16540 [01:15<1:37:08,  2.81emb/s, Current Index=188, Generated Text Snippet=<|startoftext|>\n\n|-</startand|]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 190/16540 [01:15<1:41:07,  2.69emb/s, Current Index=189, Generated Text Snippet=<|startoftext|>|<nop>=<br />|] Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 191/16540 [01:15<1:43:40,  2.63emb/s, Current Index=190, Generated Text Snippet=<|startoftext|>|<i>{1}{2}{3}{4]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 191/16540 [01:16<1:43:40,  2.63emb/s, Current Index=191, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 192/16540 [01:16<1:45:43,  2.58emb/s, Current Index=191, Generated Text Snippet=<|startoftext|>\n\n<textarea> <s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 192/16540 [01:16<1:45:43,  2.58emb/s, Current Index=192, Generated Text Snippet=<|startoftext|>\n\nA:\nProcessing EEG to Text:   1%|          | 193/16540 [01:16<1:47:17,  2.54emb/s, Current Index=192, Generated Text Snippet=<|startoftext|>\n\nA:\n- \\[ A-Z ]]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 193/16540 [01:17<1:47:17,  2.54emb/s, Current Index=193, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 194/16540 [01:17<1:43:28,  2.63emb/s, Current Index=193, Generated Text Snippet=<|startoftext|>\n\n<endofline>]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 194/16540 [01:17<1:43:28,  2.63emb/s, Current Index=194, Generated Text Snippet=<|startoftext|>\\\"></table>\n\nProcessing EEG to Text:   1%|          | 195/16540 [01:17<1:45:12,  2.59emb/s, Current Index=194, Generated Text Snippet=<|startoftext|>\\\"></table>\n\n<t]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 195/16540 [01:17<1:45:12,  2.59emb/s, Current Index=195, Generated Text Snippet=<|startoftext|>|<textarea>\n\nProcessing EEG to Text:   1%|          | 196/16540 [01:17<1:46:21,  2.56emb/s, Current Index=195, Generated Text Snippet=<|startoftext|>|<textarea>\n\n<i]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 196/16540 [01:18<1:46:21,  2.56emb/s, Current Index=196, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 197/16540 [01:18<1:50:07,  2.47emb/s, Current Index=196, Generated Text Snippet=<|startoftext|>\n\n<type>A</type]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 198/16540 [01:18<1:50:00,  2.48emb/s, Current Index=197, Generated Text Snippet=<|startoftext|>\\.[\\]\\|\\/\\/)]   Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 198/16540 [01:19<1:50:00,  2.48emb/s, Current Index=198, Generated Text Snippet=<|startoftext|>\n\n[1.1]\nProcessing EEG to Text:   1%|          | 199/16540 [01:19<1:49:50,  2.48emb/s, Current Index=198, Generated Text Snippet=<|startoftext|>\n\n[1.1]\n, <|end]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 199/16540 [01:19<1:49:50,  2.48emb/s, Current Index=199, Generated Text Snippet=<|startoftext|> [<\\\n\nProcessing EEG to Text:   1%|          | 200/16540 [01:19<1:50:30,  2.46emb/s, Current Index=199, Generated Text Snippet=<|startoftext|> [<\\\n\n|beginofr]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 201/16540 [01:19<1:50:32,  2.46emb/s, Current Index=200, Generated Text Snippet=<|startoftext|> <|endofline|><]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 201/16540 [01:20<1:50:32,  2.46emb/s, Current Index=201, Generated Text Snippet=<|startoftext|> < /start>\n\nProcessing EEG to Text:   1%|          | 202/16540 [01:20<1:33:05,  2.93emb/s, Current Index=201, Generated Text Snippet=<|startoftext|> < /start>\n\n< e]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 202/16540 [01:20<1:33:05,  2.93emb/s, Current Index=202, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|          | 203/16540 [01:20<1:38:01,  2.78emb/s, Current Index=202, Generated Text Snippet=<|startoftext|>\n\n|<textarea> |]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 204/16540 [01:20<1:40:56,  2.70emb/s, Current Index=203, Generated Text Snippet=<|startoftext|>|p;|b\" , |$|$;] Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 204/16540 [01:21<1:40:56,  2.70emb/s, Current Index=204, Generated Text Snippet=<|startoftext|>|\n\nProcessing EEG to Text:   1%|          | 205/16540 [01:21<1:43:19,  2.64emb/s, Current Index=204, Generated Text Snippet=<|startoftext|>|\n\n<textarea na]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|          | 206/16540 [01:21<1:45:26,  2.58emb/s, Current Index=205, Generated Text Snippet=<|startoftext|> 1.000000 (0x4f]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|▏         | 207/16540 [01:22<1:51:40,  2.44emb/s, Current Index=206, Generated Text Snippet=<|startoftext|>|r|s|t|v)|vmfhf]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|▏         | 207/16540 [01:22<1:51:40,  2.44emb/s, Current Index=207, Generated Text Snippet=<|startoftext|>|\n\n<\\/begin>\nProcessing EEG to Text:   1%|▏         | 208/16540 [01:22<1:40:19,  2.71emb/s, Current Index=207, Generated Text Snippet=<|startoftext|>|\n\n<\\/begin>\n.]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|▏         | 209/16540 [01:22<1:43:49,  2.62emb/s, Current Index=208, Generated Text Snippet=<|startoftext|>| <li>A</li>\" |]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|▏         | 209/16540 [01:23<1:43:49,  2.62emb/s, Current Index=209, Generated Text Snippet=<|startoftext|>|~|\n\nProcessing EEG to Text:   1%|▏         | 210/16540 [01:23<1:46:36,  2.55emb/s, Current Index=209, Generated Text Snippet=<|startoftext|>|~|\n\n[^[\\t\\d\\.]]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|▏         | 210/16540 [01:23<1:46:36,  2.55emb/s, Current Index=210, Generated Text Snippet=<|startoftext|>\n\n</|\n.| |\nProcessing EEG to Text:   1%|▏         | 211/16540 [01:23<1:48:03,  2.52emb/s, Current Index=210, Generated Text Snippet=<|startoftext|>\n\n</|\n.| |\n (\\d]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|▏         | 212/16540 [01:24<1:48:52,  2.50emb/s, Current Index=211, Generated Text Snippet=<|startoftext|> | %| | |%| %+|]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|▏         | 212/16540 [01:24<1:48:52,  2.50emb/s, Current Index=212, Generated Text Snippet=<|startoftext|>\n\nProcessing EEG to Text:   1%|▏         | 213/16540 [01:24<1:49:44,  2.48emb/s, Current Index=212, Generated Text Snippet=<|startoftext|>\n\n<linebreak>]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|▏         | 213/16540 [01:24<1:49:44,  2.48emb/s, Current Index=213, Generated Text Snippet=<|startoftext|>\\\n\nProcessing EEG to Text:   1%|▏         | 214/16540 [01:24<1:50:02,  2.47emb/s, Current Index=213, Generated Text Snippet=<|startoftext|>\\\n\n\\t\\tif(!star]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing EEG to Text:   1%|▏         | 214/16540 [01:25<1:48:14,  2.51emb/s, Current Index=213, Generated Text Snippet=<|startoftext|>\\\n\n\\t\\tif(!star]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(input_ids)  \u001b[38;5;66;03m# Explicit attention mask\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Generate text\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtext_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     54\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     56\u001b[0m generated_texts\u001b[38;5;241m.\u001b[39mappend(text)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2048\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2040\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2041\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2042\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2043\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2044\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2045\u001b[0m     )\n\u001b[1;32m   2047\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2048\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2049\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2058\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2059\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2060\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2061\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2062\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2067\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2068\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:3008\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3005\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3007\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 3008\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   3011\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1316\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1309\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1314\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1316\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1333\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1130\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1119\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1120\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1127\u001b[0m         output_attentions,\n\u001b[1;32m   1128\u001b[0m     )\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1141\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:615\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    613\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    614\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 615\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    623\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    624\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:518\u001b[0m, in \u001b[0;36mGPT2SdpaAttention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    516\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m encoder_attention_mask\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 518\u001b[0m     query, key, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    520\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_heads(query, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    521\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_heads(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:915\u001b[0m, in \u001b[0;36mTensor.split\u001b[0;34m(self, split_size, dim)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(split_size, (\u001b[38;5;28mint\u001b[39m, torch\u001b[38;5;241m.\u001b[39mSymInt)):\n\u001b[0;32m--> 915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_VF\u001b[38;5;241m.\u001b[39msplit_with_sizes(\u001b[38;5;28mself\u001b[39m, split_size, dim)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":10},{"cell_type":"code","source":"import numpy as np\n\n# Load the data\ngenerated_texts = np.load('/kaggle/input/texts/generated_texts_from_eeg.npy', allow_pickle=True)\ngenerated_captions = np.load('/kaggle/input/texts/generated_captions.npy', allow_pickle=True)\n\nprint(generated_texts)\nprint()\nprint(generated_captions)\n\n# Check alignment\nassert len(generated_texts) == len(generated_captions), \"Mismatch in dataset lengths!\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T18:56:10.443700Z","iopub.execute_input":"2024-11-24T18:56:10.444075Z","iopub.status.idle":"2024-11-24T18:56:10.484524Z","shell.execute_reply.started":"2024-11-24T18:56:10.444041Z","shell.execute_reply":"2024-11-24T18:56:10.483345Z"}},"outputs":[{"name":"stdout","text":"['<|startoftext|>|replace|</b>\\\\n\\n\\n</div>\\n.\\n I want to check if the following is valid:\\n: (startOftext) {\\n,\\n (replace)|>{},'\n '<|startoftext|>|<\\n\\n<[tag=`/|-&-][ \\\\\\\\ ]>& \\\\\\\\ / | | \\\\| \\\\>/g |> \\\\-| \\\\x1| \\\\\\\\ + \\\\\\\\ - \\\\\\\\ |'\n '<|startoftext|> | |<</start>| | <end>12|\\n\\n|<textarea>\\n\\n\\nThis is the code where I put the line.\\n \"|\" is a textarea inside the <'\n ...\n '<|startoftext|>|\\\\u003cp\\\\r\\n\\n\\\\t\\\\tset \\\\tstretch\\\\tif( \\\\\\n,|\\\\\\\\r - $:/core/wordwrap/match.txt \\\\)) { \\\\tt'\n '<|startoftext|>|<input type=\"text\" name=\"name\" value=\"description\" /> <input name=\"\" name=\"description\">Description of the entry</input> <property name=\\'description\\' value=\\'Description\\' />'\n '<|startoftext|> </head> <body> <!-- this is exactly what we want--> <link href=\"http://www.mediafire.com/?0WZBxgYJwCq8dLQp']\n\n['a small pig walking across a dirt covered ground'\n 'a small pig walking across a dirt covered ground'\n 'a small deer is sitting in a pile of hay' ...\n 'a box full of zus and zus' 'three zuons on a cutting board'\n 'two zuons on a wooden table']\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!pip install sentence_transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T18:44:07.753091Z","iopub.execute_input":"2024-11-24T18:44:07.753960Z","iopub.status.idle":"2024-11-24T18:44:17.591886Z","shell.execute_reply.started":"2024-11-24T18:44:07.753928Z","shell.execute_reply":"2024-11-24T18:44:17.590940Z"}},"outputs":[{"name":"stdout","text":"Collecting sentence_transformers\n  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (2.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.14.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.25.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.20.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence_transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\nDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence_transformers\nSuccessfully installed sentence_transformers-3.3.1\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\nfrom sentence_transformers import SentenceTransformer\n\n# Load a sentence embedding model\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Generate embeddings\ncaptions_embeddings = model.encode(generated_captions)\ntexts_embeddings = model.encode(generated_texts)\n\n# Calculate cosine similarity\nsimilarities = cosine_similarity(captions_embeddings, texts_embeddings)\naverage_similarity = similarities.diagonal().mean()\nprint(f\"Average similarity: {average_similarity:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T18:56:00.932035Z","iopub.execute_input":"2024-11-24T18:56:00.932408Z","iopub.status.idle":"2024-11-24T18:56:05.907913Z","shell.execute_reply.started":"2024-11-24T18:56:00.932378Z","shell.execute_reply":"2024-11-24T18:56:05.905380Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/517 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"291288fde3444b1e9742644464d04a6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7dc6fc37985a49428a137072c51f09df"}},"metadata":{}},{"name":"stdout","text":"Average similarity: 0.0430\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"#################################################### TRIAL 202 ##############################################################################","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T18:58:37.433486Z","iopub.execute_input":"2024-11-24T18:58:37.433875Z","iopub.status.idle":"2024-11-24T18:58:37.438428Z","shell.execute_reply.started":"2024-11-24T18:58:37.433843Z","shell.execute_reply":"2024-11-24T18:58:37.437385Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\nfrom tqdm import tqdm\n\n# Set device (using CUDA if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load EEG and text embeddings\neeg_embeddings = np.load('/kaggle/input/modified-embeddings/modified_eeg_embeddings.npy')  # Replace with correct path\ntext_embeddings = np.load('/kaggle/input/modified-embeddings/modified_text_embeddings.npy')  # Replace with correct path\n\n# Convert EEG and text embeddings to PyTorch tensors\neeg_embeddings = torch.tensor(eeg_embeddings, dtype=torch.float32).to(device)\ntext_embeddings = torch.tensor(text_embeddings, dtype=torch.float32).to(device)\n\n# Define Mapping Network (to map EEG to Text Embeddings)\nclass EEGToTextMapper(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(EEGToTextMapper, self).__init__()\n        self.network = nn.Sequential(\n            nn.Linear(input_dim, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, output_dim)\n        )\n    \n    def forward(self, eeg_embedding):\n        return self.network(eeg_embedding)\n\n# Create a model for mapping EEG to text embeddings\ninput_dim = eeg_embeddings.shape[1]  # Dimension of EEG embedding\noutput_dim = text_embeddings.shape[1]  # Dimension of text embedding\nmodel = EEGToTextMapper(input_dim, output_dim).to(device)\n\n# Optimizer and Loss\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.MSELoss()\n\n# Training loop (Mapping EEG to Text Embeddings)\nepochs = 10\nfor epoch in range(epochs):\n    epoch_loss = 0\n    model.train()\n    for eeg_batch, text_batch in DataLoader(TensorDataset(eeg_embeddings, text_embeddings), batch_size=64, shuffle=True):\n        optimizer.zero_grad()\n        predicted_text_embedding = model(eeg_batch)\n        loss = criterion(predicted_text_embedding, text_batch)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(DataLoader(TensorDataset(eeg_embeddings, text_embeddings), batch_size=64)):.4f}\")\n\n# Save the trained model\ntorch.save(model.state_dict(), 'eeg_to_text_mapping_model.pth')\n\n# Load GPT-2 for text generation\ngpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n\n# Set pad_token_id to eos_token_id for GPT-2\ngpt2_model.config.pad_token_id = gpt2_model.config.eos_token_id\n\n# Load the trained EEG-to-text mapping model\nmodel.load_state_dict(torch.load('eeg_to_text_mapping_model.pth'))\nmodel.eval()\n\n# Function to convert mapped text embeddings to human-readable text using GPT-2\ndef generate_text_from_mapped_embeddings(mapped_embeddings):\n    generated_texts = []\n    with torch.no_grad():\n        for i, embedding in enumerate(tqdm(mapped_embeddings, desc=\"Generating Text from EEG Embeddings\")):\n            input_ids = tokenizer.encode(\"<|startoftext|>\", return_tensors=\"pt\").to(device)\n            attention_mask = torch.ones_like(input_ids).to(device)\n\n            # Generate text using GPT-2\n            outputs = gpt2_model.generate(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                max_length=50,\n                num_return_sequences=1,\n                no_repeat_ngram_size=2,\n                temperature=0.7,\n                do_sample=True\n            )\n\n            text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n            generated_texts.append(text)\n\n            # Print the generated text every 10 iterations\n            if (i + 1) % 10 == 0:\n                print(f\"Generated Text at iteration {i + 1}: {text[:50]}\")  # Show first 50 characters\n\n    return generated_texts\n\n# Map EEG embeddings to text embeddings\nmapped_embeddings = []\nwith torch.no_grad():\n    for eeg_batch in DataLoader(TensorDataset(eeg_embeddings), batch_size=64):\n        eeg_batch = eeg_batch[0].to(device)\n        mapped_embeddings.append(model(eeg_batch))\n\n# Concatenate the mapped embeddings\nmapped_embeddings = torch.cat(mapped_embeddings, dim=0)\n\n# Generate text from the mapped embeddings\ngenerated_texts = generate_text_from_mapped_embeddings(mapped_embeddings)\n\n# Save the generated texts\nnp.save('generated_texts_from_eeg.npy', generated_texts)\nprint(\"Generated texts saved!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T19:15:13.509637Z","iopub.execute_input":"2024-11-24T19:15:13.510197Z","iopub.status.idle":"2024-11-24T19:16:43.357219Z","shell.execute_reply.started":"2024-11-24T19:15:13.510164Z","shell.execute_reply":"2024-11-24T19:16:43.355497Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nEpoch 1/10, Loss: 0.0008\nEpoch 2/10, Loss: 0.0008\nEpoch 3/10, Loss: 0.0008\nEpoch 4/10, Loss: 0.0008\nEpoch 5/10, Loss: 0.0007\nEpoch 6/10, Loss: 0.0007\nEpoch 7/10, Loss: 0.0007\nEpoch 8/10, Loss: 0.0007\nEpoch 9/10, Loss: 0.0007\nEpoch 10/10, Loss: 0.0007\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/2240885403.py:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('eeg_to_text_mapping_model.pth'))\nGenerating Text from EEG Embeddings:   0%|          | 0/16540 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 1/16540 [00:00<2:09:20,  2.13it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 2/16540 [00:00<1:58:07,  2.33it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 3/16540 [00:01<1:54:37,  2.40it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 4/16540 [00:01<1:53:30,  2.43it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 5/16540 [00:02<1:52:29,  2.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 6/16540 [00:02<1:52:51,  2.44it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 7/16540 [00:02<1:52:42,  2.44it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 8/16540 [00:03<1:52:20,  2.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 9/16540 [00:03<1:52:07,  2.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 10/16540 [00:04<1:51:20,  2.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Generated Text at iteration 10: <|startoftext|>|<textarea> ||</textbox> <text/>\n\n<\n","output_type":"stream"},{"name":"stderr","text":"Generating Text from EEG Embeddings:   0%|          | 11/16540 [00:04<1:53:02,  2.44it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 12/16540 [00:04<1:52:41,  2.44it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 13/16540 [00:05<1:52:25,  2.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 14/16540 [00:05<1:52:25,  2.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 15/16540 [00:06<1:52:00,  2.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 16/16540 [00:06<1:51:42,  2.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 17/16540 [00:06<1:51:31,  2.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 18/16540 [00:07<1:51:40,  2.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 19/16540 [00:07<1:51:07,  2.48it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 20/16540 [00:08<1:51:09,  2.48it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Generated Text at iteration 20: <|startoftext|>\n\n| endofline|\n (\n[{|c-}a]\n] |\n-n 1\n","output_type":"stream"},{"name":"stderr","text":"Generating Text from EEG Embeddings:   0%|          | 21/16540 [00:08<1:51:27,  2.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 22/16540 [00:08<1:45:02,  2.62it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 23/16540 [00:09<1:47:17,  2.57it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 24/16540 [00:09<1:47:51,  2.55it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 25/16540 [00:10<1:48:39,  2.53it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 26/16540 [00:10<1:51:46,  2.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 27/16540 [00:10<1:51:52,  2.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 28/16540 [00:11<1:52:04,  2.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 29/16540 [00:11<1:52:02,  2.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 30/16540 [00:12<1:51:55,  2.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Generated Text at iteration 30: <|startoftext|>|newline|<\\/endtag>\n\n<tag type=text\n","output_type":"stream"},{"name":"stderr","text":"Generating Text from EEG Embeddings:   0%|          | 31/16540 [00:12<1:52:04,  2.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 32/16540 [00:12<1:51:08,  2.48it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 33/16540 [00:13<1:51:25,  2.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 34/16540 [00:13<1:51:22,  2.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 35/16540 [00:14<1:53:20,  2.43it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 36/16540 [00:14<1:52:48,  2.44it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 37/16540 [00:15<1:52:22,  2.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 38/16540 [00:15<1:52:22,  2.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 39/16540 [00:15<1:51:57,  2.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 40/16540 [00:16<1:51:05,  2.48it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Generated Text at iteration 40: <|startoftext|>|</p> <p class=\"western\" style=3D\"f\n","output_type":"stream"},{"name":"stderr","text":"Generating Text from EEG Embeddings:   0%|          | 41/16540 [00:16<1:52:24,  2.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 42/16540 [00:17<1:51:52,  2.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 43/16540 [00:17<1:51:53,  2.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 44/16540 [00:17<1:51:35,  2.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 45/16540 [00:18<1:51:13,  2.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 46/16540 [00:18<1:31:15,  3.01it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 47/16540 [00:18<1:36:57,  2.84it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 48/16540 [00:19<1:40:44,  2.73it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 49/16540 [00:19<1:47:00,  2.57it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 50/16540 [00:20<1:49:16,  2.52it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Generated Text at iteration 50: <|startoftext|>a\n\n</endofblock>\n.|_|\n (this)\n[{\n\\t\n","output_type":"stream"},{"name":"stderr","text":"Generating Text from EEG Embeddings:   0%|          | 51/16540 [00:20<1:54:15,  2.41it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 52/16540 [00:20<1:53:28,  2.42it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 53/16540 [00:21<1:52:57,  2.43it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 54/16540 [00:21<1:52:24,  2.44it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 55/16540 [00:22<1:51:14,  2.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 56/16540 [00:22<1:51:12,  2.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 57/16540 [00:22<1:51:10,  2.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 58/16540 [00:23<1:51:15,  2.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 59/16540 [00:23<1:51:30,  2.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 60/16540 [00:24<1:51:55,  2.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Generated Text at iteration 60: <|startoftext|>|endofline|<input type=\"hidden\" nam\n","output_type":"stream"},{"name":"stderr","text":"Generating Text from EEG Embeddings:   0%|          | 61/16540 [00:24<1:52:17,  2.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 62/16540 [00:25<1:52:09,  2.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 63/16540 [00:25<1:52:34,  2.44it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 64/16540 [00:25<1:52:19,  2.44it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 65/16540 [00:26<1:52:16,  2.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 66/16540 [00:26<1:52:01,  2.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 67/16540 [00:27<1:51:49,  2.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 68/16540 [00:27<1:51:46,  2.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 69/16540 [00:27<1:51:21,  2.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 70/16540 [00:28<1:51:20,  2.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Generated Text at iteration 70: <|startoftext|>\n\n[|endof|]\n,\n.text( \" The first th\n","output_type":"stream"},{"name":"stderr","text":"Generating Text from EEG Embeddings:   0%|          | 71/16540 [00:28<1:51:22,  2.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 72/16540 [00:29<1:51:07,  2.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 73/16540 [00:29<1:51:09,  2.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 74/16540 [00:29<1:50:56,  2.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 75/16540 [00:30<1:51:04,  2.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 76/16540 [00:30<1:53:07,  2.43it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 77/16540 [00:31<1:52:30,  2.44it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 78/16540 [00:31<1:52:24,  2.44it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 79/16540 [00:31<1:51:23,  2.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 80/16540 [00:32<1:41:05,  2.71it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Generated Text at iteration 80: <|startoftext|> |\n\n< |startOfText||endofline|:ends\n","output_type":"stream"},{"name":"stderr","text":"Generating Text from EEG Embeddings:   0%|          | 81/16540 [00:32<1:44:08,  2.63it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   0%|          | 82/16540 [00:33<1:45:53,  2.59it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 83/16540 [00:33<1:48:07,  2.54it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 84/16540 [00:33<1:48:19,  2.53it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 85/16540 [00:34<1:48:33,  2.53it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 86/16540 [00:34<1:48:52,  2.52it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 87/16540 [00:35<1:49:13,  2.51it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 88/16540 [00:35<1:49:44,  2.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 89/16540 [00:35<1:49:59,  2.49it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 90/16540 [00:36<1:50:13,  2.49it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Generated Text at iteration 90: <|startoftext|>\n\n|<textarea> <span>Bag</span></tex\n","output_type":"stream"},{"name":"stderr","text":"Generating Text from EEG Embeddings:   1%|          | 91/16540 [00:36<1:50:32,  2.48it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 92/16540 [00:37<1:50:36,  2.48it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 93/16540 [00:37<1:50:47,  2.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 94/16540 [00:37<1:50:51,  2.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 95/16540 [00:38<1:50:33,  2.48it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 96/16540 [00:38<1:50:39,  2.48it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 97/16540 [00:39<1:50:28,  2.48it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 98/16540 [00:39<1:50:22,  2.48it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 99/16540 [00:39<1:50:12,  2.49it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 100/16540 [00:40<1:51:56,  2.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Generated Text at iteration 100: <|startoftext|>|<a href={{#title}}{/title}>\n\n<bloc\n","output_type":"stream"},{"name":"stderr","text":"Generating Text from EEG Embeddings:   1%|          | 101/16540 [00:40<1:53:02,  2.42it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 102/16540 [00:41<1:51:27,  2.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 103/16540 [00:41<1:51:28,  2.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 104/16540 [00:41<1:51:06,  2.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 105/16540 [00:42<1:50:44,  2.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 106/16540 [00:42<1:50:47,  2.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 107/16540 [00:43<1:50:25,  2.48it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 108/16540 [00:43<1:26:55,  3.15it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 109/16540 [00:43<1:34:06,  2.91it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 110/16540 [00:44<1:39:09,  2.76it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Generated Text at iteration 110: <|startoftext|> | | \" | </span> <span class=\"start\n","output_type":"stream"},{"name":"stderr","text":"Generating Text from EEG Embeddings:   1%|          | 111/16540 [00:44<1:44:12,  2.63it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 112/16540 [00:44<1:46:06,  2.58it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 113/16540 [00:45<1:46:56,  2.56it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 114/16540 [00:45<1:48:12,  2.53it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 115/16540 [00:46<1:48:34,  2.52it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 116/16540 [00:46<1:48:47,  2.52it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 117/16540 [00:46<1:49:08,  2.51it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 118/16540 [00:47<1:49:27,  2.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 119/16540 [00:47<1:50:08,  2.49it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 120/16540 [00:48<1:47:52,  2.54it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Generated Text at iteration 120: <|startoftext|>| ||</font></font>\n\n<ol><font face=\n","output_type":"stream"},{"name":"stderr","text":"Generating Text from EEG Embeddings:   1%|          | 121/16540 [00:48<1:49:03,  2.51it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 122/16540 [00:48<1:49:28,  2.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 124/16540 [00:49<1:25:17,  3.21it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 125/16540 [00:49<1:32:10,  2.97it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 126/16540 [00:50<1:36:35,  2.83it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 127/16540 [00:50<1:42:16,  2.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 128/16540 [00:50<1:46:14,  2.57it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 129/16540 [00:51<1:50:40,  2.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 130/16540 [00:51<1:50:56,  2.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Generated Text at iteration 130: <|startoftext|>\n\n<\\/span> <\\/div>\n\n\n</table>\\u003e\n","output_type":"stream"},{"name":"stderr","text":"Generating Text from EEG Embeddings:   1%|          | 131/16540 [00:52<1:51:01,  2.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 132/16540 [00:52<1:50:41,  2.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 133/16540 [00:53<1:49:37,  2.49it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 134/16540 [00:53<1:49:55,  2.49it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 135/16540 [00:53<1:50:08,  2.48it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 136/16540 [00:54<1:49:40,  2.49it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 137/16540 [00:54<1:49:46,  2.49it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 138/16540 [00:55<1:49:25,  2.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 139/16540 [00:55<1:49:34,  2.49it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 140/16540 [00:55<1:49:13,  2.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Generated Text at iteration 140: <|startoftext|>|<title>V-1</title></body> </html>\n\n","output_type":"stream"},{"name":"stderr","text":"Generating Text from EEG Embeddings:   1%|          | 141/16540 [00:56<1:49:00,  2.51it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 142/16540 [00:56<1:49:29,  2.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 143/16540 [00:57<1:49:17,  2.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 144/16540 [00:57<1:49:52,  2.49it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 145/16540 [00:57<1:49:57,  2.49it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 146/16540 [00:58<1:49:57,  2.48it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 147/16540 [00:58<1:50:08,  2.48it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 148/16540 [00:59<1:50:04,  2.48it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 149/16540 [00:59<1:50:43,  2.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 150/16540 [00:59<1:50:31,  2.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Generated Text at iteration 150: <|startoftext|> <|endoftitle> </|\n\n<div class=\"tex\n","output_type":"stream"},{"name":"stderr","text":"Generating Text from EEG Embeddings:   1%|          | 151/16540 [01:00<1:50:59,  2.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 152/16540 [01:00<1:52:44,  2.42it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 153/16540 [01:01<1:52:06,  2.44it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 154/16540 [01:01<1:52:21,  2.43it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 155/16540 [01:01<1:51:52,  2.44it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 156/16540 [01:02<1:51:24,  2.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 157/16540 [01:02<1:50:45,  2.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 158/16540 [01:02<1:38:40,  2.77it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 159/16540 [01:03<1:41:54,  2.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 160/16540 [01:03<1:43:52,  2.63it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Generated Text at iteration 160: <|startoftext|>\n\n< | endofline|></en>|\n, \"</en><en\n","output_type":"stream"},{"name":"stderr","text":"Generating Text from EEG Embeddings:   1%|          | 161/16540 [01:04<1:45:20,  2.59it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 162/16540 [01:04<1:47:12,  2.55it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 163/16540 [01:04<1:47:51,  2.53it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 164/16540 [01:05<1:48:32,  2.51it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 165/16540 [01:05<1:48:39,  2.51it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 166/16540 [01:06<1:48:20,  2.52it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 167/16540 [01:06<1:48:59,  2.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 168/16540 [01:07<1:49:14,  2.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 169/16540 [01:07<1:49:39,  2.49it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 170/16540 [01:07<1:49:01,  2.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Generated Text at iteration 170: <|startoftext|>| endofblock |>_|_\n\n| |_/|\n.|. |\n:|\n","output_type":"stream"},{"name":"stderr","text":"Generating Text from EEG Embeddings:   1%|          | 171/16540 [01:08<1:49:03,  2.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 172/16540 [01:08<1:49:21,  2.49it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 173/16540 [01:09<1:49:34,  2.49it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 174/16540 [01:09<1:49:57,  2.48it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 175/16540 [01:09<1:50:11,  2.48it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 176/16540 [01:10<1:50:11,  2.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 177/16540 [01:10<1:52:53,  2.42it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 178/16540 [01:11<1:52:19,  2.43it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 179/16540 [01:11<1:51:39,  2.44it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 180/16540 [01:11<1:51:04,  2.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Generated Text at iteration 180: <|startoftext|>|<br> [<*]]</*>]\n\n</ul>\n.text:\n- <b\n","output_type":"stream"},{"name":"stderr","text":"Generating Text from EEG Embeddings:   1%|          | 181/16540 [01:12<1:50:41,  2.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 182/16540 [01:12<1:39:08,  2.75it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 183/16540 [01:12<1:42:13,  2.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 184/16540 [01:13<1:44:36,  2.61it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 185/16540 [01:13<1:46:06,  2.57it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 186/16540 [01:14<1:47:05,  2.55it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 187/16540 [01:14<1:47:49,  2.53it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 188/16540 [01:14<1:47:49,  2.53it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 189/16540 [01:15<1:48:21,  2.52it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 190/16540 [01:15<1:49:12,  2.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Generated Text at iteration 190: <|startoftext|>=.<*></start> <endofline>$1</endlin\n","output_type":"stream"},{"name":"stderr","text":"Generating Text from EEG Embeddings:   1%|          | 191/16540 [01:16<1:49:09,  2.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 192/16540 [01:16<1:49:32,  2.49it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 193/16540 [01:16<1:49:21,  2.49it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 194/16540 [01:17<1:49:18,  2.49it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 195/16540 [01:17<1:49:32,  2.49it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 196/16540 [01:18<1:49:22,  2.49it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 197/16540 [01:18<1:49:44,  2.48it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 198/16540 [01:18<1:49:44,  2.48it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 199/16540 [01:19<1:49:54,  2.48it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 200/16540 [01:19<1:50:15,  2.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Generated Text at iteration 200: <|startoftext|>\n\n</style> </head> <body> { \"title\"\n","output_type":"stream"},{"name":"stderr","text":"Generating Text from EEG Embeddings:   1%|          | 201/16540 [01:20<1:50:31,  2.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 202/16540 [01:20<1:52:29,  2.42it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 203/16540 [01:21<1:51:32,  2.44it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 204/16540 [01:21<1:51:07,  2.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 205/16540 [01:21<1:51:05,  2.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 206/16540 [01:22<1:50:22,  2.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nGenerating Text from EEG Embeddings:   1%|          | 206/16540 [01:22<1:48:52,  2.50it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 112\u001b[0m\n\u001b[1;32m    109\u001b[0m mapped_embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(mapped_embeddings, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Generate text from the mapped embeddings\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m generated_texts \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_text_from_mapped_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapped_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Save the generated texts\u001b[39;00m\n\u001b[1;32m    115\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_texts_from_eeg.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, generated_texts)\n","Cell \u001b[0;32mIn[16], line 82\u001b[0m, in \u001b[0;36mgenerate_text_from_mapped_embeddings\u001b[0;34m(mapped_embeddings)\u001b[0m\n\u001b[1;32m     79\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(input_ids)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Generate text using GPT-2\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mgpt2_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     90\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     93\u001b[0m generated_texts\u001b[38;5;241m.\u001b[39mappend(text)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2048\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2040\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2041\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2042\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2043\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2044\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2045\u001b[0m     )\n\u001b[1;32m   2047\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2048\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2049\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2058\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2059\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2060\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2061\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2062\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2067\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2068\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:3008\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3005\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3007\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 3008\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   3011\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1316\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1309\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1314\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1316\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1333\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1130\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1119\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1120\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1127\u001b[0m         output_attentions,\n\u001b[1;32m   1128\u001b[0m     )\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1141\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:614\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    604\u001b[0m     hidden_states: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    611\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    612\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor], Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]]:\n\u001b[1;32m    613\u001b[0m     residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 614\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    615\u001b[0m     attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\n\u001b[1;32m    616\u001b[0m         hidden_states,\n\u001b[1;32m    617\u001b[0m         layer_past\u001b[38;5;241m=\u001b[39mlayer_past,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    621\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    622\u001b[0m     )\n\u001b[1;32m    623\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/normalization.py:202\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2576\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2573\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2574\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2575\u001b[0m     )\n\u001b[0;32m-> 2576\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":16},{"cell_type":"code","source":"import torch\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nimport numpy as np\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tqdm import tqdm\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Initialize DataParallel if multiple GPUs are available\nif torch.cuda.device_count() > 1:\n    print(\"Using 2 GPUs!\")\n    # Wrap the model with DataParallel to utilize both GPUs\n    blip_model = torch.nn.DataParallel(BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")).to(device)\nelse:\n    # Use single GPU or CPU\n    blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n\n# Load the BLIP processor\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\n# Load EEG embeddings (Replace with your actual path)\neeg_embeddings = np.load('/kaggle/input/modified-embeddings/modified_eeg_embeddings.npy')\neeg_embeddings = torch.tensor(eeg_embeddings, dtype=torch.float32).to(device)\n\n# Normalize EEG data to the range [0, 1] if not already done\neeg_embeddings = (eeg_embeddings - eeg_embeddings.min()) / (eeg_embeddings.max() - eeg_embeddings.min())\n\n# Reshape EEG embeddings to (batch_size, 1, 1, num_features) for BLIP input\neeg_embeddings = eeg_embeddings.unsqueeze(1).unsqueeze(2)  # Adding 2 singleton dimensions for image-like data\n\n# Duplicate across 3 channels to simulate RGB images\neeg_embeddings_3channel = eeg_embeddings.repeat(1, 3, 1, 1)  # (batch_size, 3, height, width)\n\n# DataLoader to process EEG data in batches\ndataset = TensorDataset(eeg_embeddings_3channel)\ndataloader = DataLoader(dataset, batch_size=8, shuffle=False)\n\n# List to store generated captions\ngenerated_texts = []\n\n# Generate text from EEG embeddings using BLIP\nprint(\"Starting text generation using BLIP...\")\nwith tqdm(total=len(eeg_embeddings), desc=\"Processing EEG to Text\", unit=\"batch\") as pbar:\n    for i, batch in enumerate(dataloader):\n        eeg_batch = batch[0].to(device)\n\n        # Use BLIP to generate captions (text)\n        inputs = processor(images=eeg_batch, return_tensors=\"pt\", do_rescale=False).to(device)  # Process EEG data as image input\n        \n        # Use blip_model.module if DataParallel is used\n        output = blip_model.module.generate(**inputs, max_length=50, num_beams=5, temperature=1.0)  # Generate captions\n\n        # Decode and store generated text\n        caption = processor.decode(output[0], skip_special_tokens=True)\n        generated_texts.append(caption)\n\n        # Display the caption at every 10 iterations\n        if (i + 1) % 10 == 0:\n            print(f\"Generated Text at iteration {i + 1}: {caption[:50]}...\")  # Show first 50 characters\n\n        pbar.update(1)\n\n# Save generated captions\nnp.save(\"generated_texts_from_eeg.npy\", generated_texts)\nprint(\"Generated texts saved!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T19:34:21.175576Z","iopub.execute_input":"2024-11-24T19:34:21.175928Z","iopub.status.idle":"2024-11-24T19:37:05.424160Z","shell.execute_reply.started":"2024-11-24T19:34:21.175898Z","shell.execute_reply":"2024-11-24T19:37:05.422836Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nUsing 2 GPUs!\nStarting text generation using BLIP...\n","output_type":"stream"},{"name":"stderr","text":"Processing EEG to Text:   0%|          | 10/16540 [00:34<15:47:19,  3.44s/batch]","output_type":"stream"},{"name":"stdout","text":"Generated Text at iteration 10: a black and white photo of a man in a suit and tie...\n","output_type":"stream"},{"name":"stderr","text":"Processing EEG to Text:   0%|          | 20/16540 [01:09<16:36:46,  3.62s/batch]","output_type":"stream"},{"name":"stdout","text":"Generated Text at iteration 20: a black and white photo of a man in a suit and tie...\n","output_type":"stream"},{"name":"stderr","text":"Processing EEG to Text:   0%|          | 30/16540 [01:46<16:53:12,  3.68s/batch]","output_type":"stream"},{"name":"stdout","text":"Generated Text at iteration 30: a black and white photo of a man in a suit and tie...\n","output_type":"stream"},{"name":"stderr","text":"Processing EEG to Text:   0%|          | 40/16540 [02:23<16:33:53,  3.61s/batch]","output_type":"stream"},{"name":"stdout","text":"Generated Text at iteration 40: a black and white photo of a man in a suit and tie...\n","output_type":"stream"},{"name":"stderr","text":"Processing EEG to Text:   0%|          | 45/16540 [02:42<16:33:37,  3.61s/batch]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[27], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(images\u001b[38;5;241m=\u001b[39meeg_batch, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, do_rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Process EEG data as image input\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Use blip_model.module if DataParallel is used\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mblip_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Generate captions\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Decode and store generated text\u001b[39;00m\n\u001b[1;32m     56\u001b[0m caption \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/blip/modeling_blip.py:1196\u001b[0m, in \u001b[0;36mBlipForConditionalGeneration.generate\u001b[0;34m(self, pixel_values, input_ids, attention_mask, interpolate_pos_encoding, **generate_kwargs)\u001b[0m\n\u001b[1;32m   1193\u001b[0m input_ids[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtext_config\u001b[38;5;241m.\u001b[39mbos_token_id\n\u001b[1;32m   1194\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m attention_mask[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1196\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msep_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2079\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2071\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2072\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2073\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2074\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2075\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2076\u001b[0m     )\n\u001b[1;32m   2078\u001b[0m     \u001b[38;5;66;03m# 13. run beam sample\u001b[39;00m\n\u001b[0;32m-> 2079\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2081\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2082\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2083\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2084\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2085\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2086\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2087\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2089\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[1;32m   2090\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2091\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2092\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2093\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2099\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2100\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:3315\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3312\u001b[0m next_tokens \u001b[38;5;241m=\u001b[39m next_tokens \u001b[38;5;241m%\u001b[39m vocab_size\n\u001b[1;32m   3314\u001b[0m \u001b[38;5;66;03m# stateless\u001b[39;00m\n\u001b[0;32m-> 3315\u001b[0m beam_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mbeam_scorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3316\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnext_token_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnext_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnext_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3321\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeam_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeam_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_prompt_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_prompt_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3324\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3326\u001b[0m beam_scores \u001b[38;5;241m=\u001b[39m beam_outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_beam_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   3327\u001b[0m beam_next_tokens \u001b[38;5;241m=\u001b[39m beam_outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_beam_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/beam_search.py:255\u001b[0m, in \u001b[0;36mBeamSearchScorer.process\u001b[0;34m(self, input_ids, next_scores, next_tokens, next_indices, pad_token_id, eos_token_id, beam_indices, group_index, decoder_prompt_len)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[1;32m    254\u001b[0m     batch_group_idx \u001b[38;5;241m=\u001b[39m batch_idx \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_beam_groups \u001b[38;5;241m+\u001b[39m group_index\n\u001b[0;32m--> 255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_done[batch_group_idx]:\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_beams \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_beam_hyps[batch_group_idx]):\n\u001b[1;32m    257\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch can only be done if at least \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_beams\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m beams have been generated\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":27}]}